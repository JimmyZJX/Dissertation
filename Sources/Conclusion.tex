%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion and Future Work}
\label{chap:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion}

In this thesis, we proposed new bidirectional type inference algorithms for
predicative higher-ranked implicit parametric polymorphic systems.
We showed how worklists ease the design of algorithms
and also mechanical formalizations.
By collecting all judgments in a single worklist,
the algorithm performs unification with a bird's-eye view of all the judgments,
therefore propagation between judgments is as simple as a global substitution.
Compared with classical HM unification procedure,
Our algorithm does not need separated relations (or fixpoints) for unification.
From the formalization point of view,
eager substitutions are also easier to state and reason in a proof assistant.
Therefore, we obtained fully formalized properties for all our developments
relatively easily.
Overall, we developed the following systems and/or type inference algorithms:

\begin{itemize}
    \item We developed a worklist algorithm for OL's higher-ranked subtyping system.
        The algorithm operates on a worklist of subtyping judgments and
        a single context where a variable declaration is shared across the worklist.
        Compared with DK's algorithm, our approach avoids the use of output contexts,
        which complicates the scoping of variables and is hard to formalize
        in a proof assistant.
        Eager substitutions that solve existential variables are directly applied to
        the worklist, therefore passing the partial information to the rest judgments.
        We proved \emph{soundness}, \emph{completeness} and \emph{decidability} of
        the algorithm in the Abella theorem prover.

    \item We developed a worklist algorithm for DK's higher-ranked bidirectional type system.
        In order to properly encode judgments that
        output types, such as the type inference judgment,
        continuation-passing-style \emph{judgment chains} are developed.
        Compared with the previous work that uses a single context,
        we further unify the worklist with variable declarations.
        Such unification results in a much more accurate track of variable scopings,
        and the algorithm will garbage-collect variables
        as soon as they are not referred to anymore.
        Unlike using output contexts as DK's algorithm,
        designing rules for worklist context is less likely to contain bugs
        in terms of variable scoping.
        Once again, based on eager substitutions, the algorithm is easy to formalize.
        We showed \emph{soundness}, \emph{completeness} and \emph{decidability}
        in the Abella theorem prover.

    \item We developed a backtracking-based algorithm for a higher-ranked
        bidirectional type system with object-oriented subtyping.
        With the introduction of the top and bottom types and relevant subtyping relations,
        meta-variable instantiations are no longer deterministic like the HM system.
        The backtracking-based algorithm preserves most characteristics of the previous
        worklist context, and it ``tries'' obvious solutions in parallel with detailed analysis
        as previous work.
        We proved that the algorithm is always \emph{sound},
        and subtyping is complete under the rank-1 restriction.
\end{itemize}


\section{Future Work}

In this section, we discuss several interesting possibilities to explore in the future.

\paragraph{Termination Theorem for Type Inference with Subtyping}
The algorithm we presented and formalized in Chapter~\ref{chap:Top}
enjoys several important machine-verified properties,
including soundness and completeness under a rank-1 restriction.
However, we have not yet found a proper termination measure
for the algorithmic subtyping rules.
What complicates the problem is that
common measures are not decreasing all the time.
For example, subtyping between function types may increase the number of judgments;
and instantiation judgments that split existential variables may increase the number of
unsolved existential variables.
We are mostly convinced by the fact that
a large set of tests performed on randomly generated
subtyping judgments do terminate.
And we hope that a clever termination argument may be proposed in the near future.

\paragraph{Practical Impredicative Type Inference}
Impredicative type inference algorithms are sometimes helpful when dealing with
higher-ranked types.
Simple forms of impredicativity can be unambiguous and easy to understand by programmers,
such as
\begin{verbatim}
  head ids
\end{verbatim}
where \verb|head| $ :: \all[p] [p] \to p$ and \verb|ids| $ :: [\all a \to a]$.
The application (\verb|head ids|) should have type $\all a \to a$,
because $p$ is instantiated to $\all a \to a$ and that is the only choice we can make.

Moreover, even though there are programs that have incomparable System F types,
impredicative system may offer means like type annotation to manually pick the right one.
For example,
\begin{verbatim}
  single id
\end{verbatim}
where \verb|single| $ :: \all p \to [p]$ and \verb|id| $ :: \all a \to a$.
This term has two incomparable types $[\all a \to a]$ and $\all[a] [a \to a]$.
One might expect that a type annotation might help pick the right case,
like (\verb|(single id)| $ :: [\all a \to a]$),
which requires a system that allows impredicative instantiation to do so.
There are many impredicative systems developed out of various techniques~
\cite{le2003ml,leijen2008hmf,leijen2009flexible,vytiniotis2008fph,
Serrano2018,quicklook2020,FreezeML},
and we leave the extension of impredicativity to our algorithms as future work.

% \paragraph{Elaboration}
% \jimmy{Already discussed before... consider removing}

\paragraph{Optimization}

All the three type inference algorithms described
in this thesis are based on eager global substitution on the worklists.
The benefit is to reduce the difficulty for mechanical formalizations
in theorem provers, especially in Abella.
Additionally, eager substitutions represent simple equivalent transformations
on the state of worklist,
alleviating the complication of variable scoping and reasoning for correctness.
However, a naive implementation of our algorithm will be very inefficient,
since the worklist is iterated so often
whenever an existential variable is solved or partially solved.

Compared with mature algorithms like the algorithm $\mathcal{W}$~\citep{milner1978theory}
and the OutsideIn(X)~\citep{outsidein} type inference algorithm,
which produces substitutions once all the constraints are collected and reduced,
our algorithms manipulate the judgments themselves on-the-fly.
Efficient implementations will model meta-variables as mutable references
and apply the results if they are solved by the constraint solver.
DK's algorithm~\citep{dunfield2013complete} requires output contexts,
which are likely to be implemented in a state monad or other stateful approaches.
Solved existential variable is encoded in their algorithmic context different from ours.
Instead of removing the declaration of the variable and propagate its solution to
all the rest judgments, they extend the entry in the context with its solution,
$\al = \tau$, and pass it to the output context.
Such entry is easy to implement with an efficient approach using mutable references.
We can also adopt such an idea and reformulate our eager substitution with
lazy ones by keeping track of solutions to existential variables,
and only perform substitution when the judgment is just about to be reduced.

Unfortunately, lazy substitution only solves part of the problem.
By using ordered contexts, the scoping of variables is precisely captured by
the relative positions in which they are declared in the context.
Some algorithmic rules are designed to use the position information,
where swapping positions might lead to unsound implementation.
For example, the algorithmic system of Chapter~\ref{chap:Top}
include these rules:
\begin{gather*}
\begin{aligned}
    \Gm[a][\bt] \Vdash a \le \bt &\rrule{16} \{\bt := a\}~\Gm[a][\bt]\\
    \Gm[a][\bt] \Vdash \bt \le a &\rrule{17} \{\bt := a\}~\Gm[a][\bt]\\
\end{aligned}
\end{gather*}
The hole notation $\Gm[a][\bt]$ is used to ensure that $a$ is declared before $\bt$,
therefore it is fine for the monotype represented by $\bt$ to be $a$.
In a naive implementation, frequent iteration is required to look up the
relative positions of variable declarations.

Moreover, there are also reduction rules that \emph{remove}
and/or \emph{insert} variables from(to) the middle of the ordered context,
as occurred in the following rules:
\begin{gather*}
\begin{aligned}
\Gm[\al] \Vdash \al \le A\to B &\rrule{14}
    \{\al := \al[1] \to \al[2]\}~(\Gm[\al] \Vdash \al \le A\to B)\\
    &\qquad\qquad \text{when } \al\notin FV(A \to B)\\
\Gm[\al] \Vdash A\to B \le \al &\rrule{15}
    \{\al := \al[1] \to \al[2]\}~(\Gm[\al] \Vdash A\to B \le \al)\\
    &\qquad\qquad \text{when } \al\notin  FV(A \to B)\\
\end{aligned}
\end{gather*}

To sum up,
we plan to propose an implementation-friendly version of the algorithm
that does not require many eager substitutions,
and we also hope that there will be suitable data structures or algorithms
which optimizes the complexity of those time-consuming operations on the ordered context.


\paragraph{Class Hierarchy of Nominal Subtyping}
Nowadays, most mainstream object-oriented programming languages
support nominal subtyping,
which means that the subtyping relation is defined by the programmers explicitly
through language constructs such as inheritance.
For example, we may define a $\text{Pos}$ class to represent positive integers,
which inherits the $\text{Int}$ class.
The subtyping relation $\text{Pos} \le \text{Int}$ automatically holds.
An algorithmic subtyping judgment $\al \le \text{Int}$
now might have several solutions: $\text{Int}, \text{Pos},$ or $\bot$.
In order to calculate a \emph{range} of solutions instead of
just simple types,
lazy algorithms that operate on bounds fit the task better.
We plan to explore extensions of the algorithm described in Section~\ref{subsec:lazy_subst}.
The algorithm progressively collects upper and lower bounds of existential variables,
and finally checks if valid solutions are satisfying all the bounds.

Nonetheless, we encounter termination problems when trying to adopt the lazy approach.
Experiments indicate that loop dependency in combination with the
instantiation rule ($\al \le A \to B$) might cause infinite loops.
As a practical compromise,
we may impose restrictions on existential variables
so that they are not instantiated to function types
when they are likely to be a class type.
We also hope that better treatment on loop dependencies can be
developed, especially under assumptions of real-world programming tasks.
For future work,
we aim at a terminating and sound algorithm for practical object-oriented type inference.


\paragraph{Type Inference with Recursive Types}
Among all the algorithms we proposed in this thesis,
we reject subtyping judgments like
$$\al \le A \to B \text{ where } \al \in \text{FV}(A) \cup \text{FV}(B)$$
because no HM monotype satisfies a subtyping relation like
$$\tau \le \tau \to \text{Int}$$
and unfolding such algorithmic judgment might cause an infinite loop.
However, recursive types may satisfy such judgment, for example,
$$\mu x.~x \to \text{Int} \le (\mu x.~x \to \text{Int}) \to Int$$
holds since the unfolding of $\mu x.~x \to \text{Int}$
is $(\mu x.~x \to \text{Int}) \to \text{Int}$.
In other words, if we extend our type system by including recursive types,
we would accept such judgment instead of rejecting it.
The MLsub~\citep{mlsub} already accepts recursive types,
and it deals with recursive constraints in the way we described above.
It is an interesting question
if adding recursive types may improve the expressiveness
of our system and remain complete at the same time.

\paragraph{Bounded Quantification and F-Bounded Quantification}
Bounded quantification
\\ \citep{cardelli1985bounded} and
F-bounded quantification~\citep{canning1989fbounded} are
techniques commonly seen in object-oriented programming languages.
Bounded quantification allows constraints imposed on
universally quantified variables:
$$\all[(a \le \text{Int})] a \to a \to \text{String}$$
F-Bounded quantification further extends the constraints so that
recursive references of the variables are also allowed.
Although the subtyping is proven to be undecidable
in presence of both F-bounded quantification and variance on generics,
\citet{kennedy2007on} proposed three forms of restrictions that
we can explore further.
Combining bidirectional type checking with bounded quantifications
may further improve the experience for object-oriented programmers.


