\section{Related Work}

\paragraph{Type Inference for Polymorphic Subtyping}

Higher-order polymorphism is a practical and important programming language
feature. Due to the undecidability of type-inference for System F~\cite{wells1999typability},
different decidable partial type-inference approaches were developed. The subtyping
relation of this paper, originally proposed by Odersky and
La\"ufer~\cite{odersky1996putting}, is \emph{predicative} ($\forall$'s only
instantiate to monotypes), which is considered a reasonable and practical
trade-off. There is also work on partial impredicative type-inference
algorithms~\cite{le2003ml,leijen2008hmf,vytiniotis2008fph}.
However, unlike the predicative subtyping relation for System F, 
the subtyping for impredicative System F is
undecidable~\cite{tiuryn1996subtyping}. Therefore such algorithms 
have to navigate through the design space to impose restrictions that
allow for a decidable algorithm. As a result such algorithms tend to
be more complex, and are less adopted in practice.

Gundry et al.~\cite{gundry2010type} revisited the Hindley-Milner type system. They make use of ordered contexts on the unification during type inference, and their algorithm works differently from algorithm $\mathcal{W}$.
Dunfield and Krishnaswami~\cite{dunfield2013complete} adopted a similar idea on ordered contexts and presented an algorithmic approach for predicative polymorphic subtyping
that tracks the (partial) solutions of existential variables in the algorithmic
context---this denotes a delayed substitution that is incrementally applied
to outstanding work as it is encountered.  
%Instead of reifying the substitution, our algorithm keeps track of an explicit list of
%outstanding work.
Their algorithm comes with 40 pages of manual proofs on the soundness, completeness
and decidability. We have tried to
mechanize these proofs directly, but have not been successful yet because most proof assistants
do not naturally support output contexts and their more complex
ordered contexts.
Their theorems have statements that are more complex than those in the worklist
approach. One of the reasons for the added complexity is that, when the
constraints are not strict enough, the algorithm may not instantiate all
existential variables. However in order to match the declarative judgement,
all the unsolved variables should be properly assigned.
For example, their generalized completeness theorem is:
\begin{theorem}[Generalized Completeness of Subtyping \cite{dunfield2013complete}]~\\
If $\Psi\longrightarrow \Phi$ and $\Psi\vdash A$ and $\Psi\vdash B$ and $[\Phi]\Psi\vdash [\Phi]A \le [\Phi]B$ then there exist $\Delta$ and $\Phi'$ such that $\Delta\longrightarrow\Phi'$ and $\Phi\longrightarrow\Phi'$ and $\Psi\vdash[\Psi]A <: [\Psi]B \dashv \Delta$.
\end{theorem}
Here, the auxiliary relation $\Psi\longrightarrow \Psi'$ extends a context
$\Psi$ to a context $\Psi'$. This is used to extend the algorithm's input and
output contexts $\Psi$ and $\Delta$, with with possibly unassigned existential
variables, to a complete (i.e., fully-assigned) contexts $\Phi$ and $\Phi'$ 
suitable for the declarative specification.

While we are faced with a similar gap between algorithm and specification,
which we tackle with our transfer relations $\Gamma \to \Psi$, our completeness
statement is much shorter because our algorithm does not return an output context
which needs to be transferred. Moreover, we have cleanly encapsulated any
substitutions to the worklist in the worklist transfer judgement $\Gamma \mid
\exps \rightsquigarrow \exps'$.

Peyton Jones et al.~\cite{jones2007practical} developed a higher-rank
predicative bidirectional type system.  They enriched their subtyping relations
with deep skolemisation, while other relations remain similar to ours.  Their
algorithm is unification-based with a structure similar to algorithm
$\mathcal{W}$'s.
%The soundness and completeness properties are shown through a hand-written
%proof in 80 pages, which might lack extensibility and is less convincing
%compared with a machine-checked one.
%Unification, though widely used in type inference algorithms,
%hardly appears in proof assistants due to the difficulty of encoding and
%reasoning.

\paragraph{Unification Algorithms} Our algorithm works similarly to
some unification algorithms that use a set of unification constraints
and single-step simplification transitions. Some
work~\cite{Reed2009,Abel2011higher} adopts this idea in dependently typed
inference and reconstruction. These approaches collect a set of
constraints and nondeterministically process one of them at a
time. Those approaches consider
various forms of constraints, including term
unification, context unification and solution for meta-variables. In
contrast, our algorithm is presented in a simpler form, using ordered
(worklist) judgements, which is sufficient for the subtyping problem.

\paragraph{Formalizations of Type-Inference Algorithms in Theorem Provers}
The well-known \textsc{POPLMark} challenge~\cite{aydemir2005mechanized} has
encouraged the development of new proof assistant features for facilitating the
development and verification of type systems.
As a result, many theorem provers and packages now provide methods for dealing
with variable binding~\cite{aydemir2008engineering,urban2008nominalTech,chlipala2008parametric},
and more and more type system designers choose to formalize their proofs with these tools.
Yet, difficulties with mechanising algorithmic aspects, like unification and
constraint solving, have received very little attention. Moreover, while most
type system judgements only feature local (input) contexts, which have a simple
binding/scoping structure, many traditional type-inference algorithms require
more complex binding structures with output contexts.

Naraschewski and Nipkow~\cite{naraschewski1999type} published the first formal
verification of algorithm $\mathcal{W}$ in
Isabelle/HOL~\cite{nipkow2002isabelle}. The treatment of new variables is a little
tricky in their formalization, while most other parts follow the
structure of Damas's manual proof closely.
%Another complication is the encoding of substitutions,
%which they chose to formalize as a function instead
%of an association list.
Following Naraschewski and Nipkow other researchers~\cite{dubois2000proving,dubois1999certification} prove
a similar result in Coq~\cite{Coq}. Nominal
techniques~\cite{urban2008nominalTech} in Isabelle/HOL have also been used for a similar
verification~\cite{urban2008nominal}. Moreover, Garrigue~\cite{garrigue2015certified}
mechanized a type inference algorithm for Core ML extended with structural
polymorphism and recursion.

% These are not type-inference algorithm formalizations
% Besides, formal verification have been done on various programming languages. Due to the complexity of modern languages, usually only the core of each language is formalized, such as Java-light\cite{nipkow1998java}, "a subset of Java"\cite{drossopoulou1997java}, and DOT for Scala\cite{}.



\begin{comment}
List of (possible) citations:
\begin{itemize}
%	\item T. Rompf and N. Amin. From F to DOT: Type soundness proofs with definitional interpreters. Technical report, Purdue University and EPFL, 2016.
	\item Guarded impredicative polymorphism (about impredicative polymorphism)
\end{itemize}
\end{comment}

