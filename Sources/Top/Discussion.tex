
\section{Discussion}

% \subsection{Design of Various Backtracing Patterns}

\subsection{A Complete Algorithm Under Monotype Guessing Restrictions}

The incompleteness of the algorithm is mainly due to incomplete guesses
for existential variable instantiations when dealing with subtyping.
For example, when reducing the algorithmic subtyping judgment
$$\al \le \al \to 1$$
we only consider the possibility $\al := \bot$
and stop reducing because the judgment involves a self reference.
In fact, there are infinitely many valid solutions just in our type system,
including $\al := \top \to \bot$ and $\al := \top \to 1$.
The problem can be solved by introducing recursive types,
so that recursive constraints can be solved.
However, the other type of incompleteness that comes from
$$\al \le \bt$$
is still not addressed.

We propose an alternative approach that requires little change to the type system
by restricting the declarative system.
If the declarative system did not treat the bottom and top types
as monotypes in the first place,
the algorithm would not need to guess those types.
Formally, if monotypes are defined as follows,
$$\text{Monotypes'}\qquad \tau' \quad  ::= \quad 1 \mid a \mid \tau_1\to \tau_2$$
then subtyping relation between monotypes is simply equality.

The incomplete examples mentioned above can be addressed now,
judgments like $\al \le \al \to 1$ can no longer instantiate $\al$ to a function type,
otherwise it will infinitely loop without emitting any valid solution;
judgments like $\al \le \bt$ can now be safely treated as $\al = \bt$,
because the instantiations of $\al$ and $\bt$ must be equal to each other.
Formally speaking, we proved the following lemma
\begin{lemma}[Subtyping between Monotypes is Equality]~\\
    If $\Psi \vdash \tau'_1 \le \tau'_2$, then $\tau'_1 = \tau'_2$.
\end{lemma}
which also holds for the system in Chapter~\ref{chap:ICFP},
yet it does not hold for the system with subtyping when monotypes are not restricted.

On the meantime, algorithmic subtyping rules 12 and 13, and typing rules 27 and 37
should be removed from the algorithm under the monotype restriction,
since these rules solves existential variables to $\top$ or $\bot$.
Other than that, the algorithm remains unchanged.
Note that after removing these rules, the algorithm no longer have overlapping rules,
therefore backtracking is not needed any more.

Following the proof techniques in Chapter~\ref{chap:ICFP},
both \emph{soundness} and \emph{completeness} theorems
are successfully proven and also mechanized.
Given that the algorithm reduce judgments in a similar way to the one in the previous chapter,
the result is not so surprising.
However, it still reveals two facts:
firstly, if we do not try to guess any type involving subtyping relation
(the top and bottom types in our case),
the algorithm should remain simple and complete;
secondly, although the current algorithm for the system with subtyping is incomplete,
all source of incompleteness comes from subtyping relations.

\subsection{Lazy Substitution and Non-terminating Loops}\label{subsec:lazy_subst}
In this substction, we propose a possible way to fix the incompleteness issue
for the subtyping part of the algorithm and discuss its impact to the existing formalization.

As we have analyzed before,
one major source of incompleteness comes from rule 20 and 21,
where two different existential variables are compared.
A natural idea is to delay the solving procedure until all the corresponding
constraints are collected.
Constraint collection can be implemented by the following new worklist definition,
where the existential variables also come with a set of upper bounds
and a set of lower bounds. Both of the bounds consist of algorithmic mono-types.

\begin{gather*}
    \begin{aligned}
        \text{Algorithmic worklist}\qquad&\Gm &::=&\quad \cdot \mid \Gm, a \mid \Gm, \overline{\tau_A} \le \al \le \overline{\tau_A} \mid \Gm \Vdash \jg
    \end{aligned}
\end{gather*}

\paragraph{Differences of Algorithmic Rules.}

In presence of the bound collections, the algorithm behaves slightly differently
on variable solving rules.
Firstly, rules that eagerly solve existential variables to the top or bottom type
are discarded (Rules 12 and 13).
Next, Rules 20 and 21 add the corresponding existential variable
as part of a bound rather than performing a global substitution.

\begin{gather*}
    \begin{aligned}
\Gm[\al][L \le \bt \le U] \Vdash \al \le \bt &\rrule{20'}
    \Gm[\al][L \cup \{\al\} \le \bt \le U]\\
\Gm[\al][L \le \bt \le U] \Vdash \bt \le \al &\rrule{21'}
    \Gm[\al][L \le \bt \le U \cup \{\al\}]\\
    \end{aligned}
\end{gather*}

Instantiation rules (Rules 14 and 15) also behave lazily in order not to lose completeness.
\begin{gather*}
    \begin{aligned}
\Gm[L \le \al \le U] &\Vdash \al \le A \to B \rrule{14'}\\
    & \Gm[\al[1], \al[2], L \le \al \le U \cup \{\al[1] \to \al[2]\}] \Vdash \al[1] \to \al[2] \le A \to B\\
\Gm[L \le \al \le U] &\Vdash A \to B \le \al \rrule{15'}\\
    & \Gm[\al[1], \al[2], L \cup \{\al[1] \to \al[2]\} \le \al \le U] \Vdash A \to B \le \al[1] \to \al[2]\\
    \end{aligned}
\end{gather*}
These new rules differ from the original ones:
the two fresh existential variables representing a function type $\al[1] \to \al[2]$
now acts as a bridge between $\al$ and $A \to B$.
The algorithm breaks the judgment $\al \le \al[1] \to \al[2]$ into two sub-problems:
$\al \le \al[1] \to \al[2]$ and $\al[1] \to \al[2] \le A \to B$.
And it is not difficult to prove that the problem before and after
such transformation are equivalent to each other.

The change of existential variable declaration and solving mechanisms
also result in the change of
the algorithmic rule for garbage collecting them.
When $\al$ is at the top of the worklist and thus going to be recycled,
we need to further check if it can actually be solved,
by ensuring that lower bounds are indeed subtypes of upper bounds:
any type that is a subtype of $\al$ should be subtype of
any super type of $\al$.

$$
\Gm, \{l_i\}^n \le \al \le \{u_j\}^m \rrule{2'} \Gm \Vdash_{1 \le i \le n, 1 \le j \le m} l_i \le u_j
$$

The notion $\{l_i\}^n$ indicates that the bound collection is of length $n$;
the notion
$\Vdash_{1 \le i \le n, 1 \le j \le m} l_i \le u_j$
is similar to the syntax of list-comprehension, which creates a list of judgments
$$\Vdash l_1 \le u_1 \cdots \Vdash l_1 \le u_m \cdots \Vdash l_2 \le u_1
\cdots \Vdash l_n \le u_m$$
According to the transitivity of subtyping, this rule is sound:
any valid instantiation of $\al$, denoted $\tau_{\al}$, indicates that
the instantiation of $\l_i \le \tau_{\al}$ and $\tau_{\al} \le $ holds,
therefore the instantiation of $l_i \le u_j$ holds for any $1 \le i \le n, 1 \le j \le m$.

On the other hand, such check is sufficient. In other words,
we can always find a proper solution for $\al$ if any lower bound
is subtype of any upper bound.
This is supported by the following property on declarative monotypes:
\begin{lemma}[Sufficient Bound Check for Monotypes]~\\
    Given collections of monotypes $\{l_i\}^n$ and $\{u_j\}^m$,\\
    if $\Psi \vdash l_i \le u_j$ holds for any $1 \le i \le n, 1 \le j \le m$,\\
    then there exists $\tau$ s.t. $\Psi \vdash l_i \le \tau$ and $\Psi \vdash \tau \le u_i$
    for any $1 \le i \le n, 1 \le j \le m$.
\end{lemma}

\paragraph{Impact of the New Algorithm.}

The new algorithm addresses the incompleteness issue throughly by
re-designing in a lazy-substitution style.
However, the algorithm might loop forever due to the
new version of instantiation rules.
For example, the following derivation results in infinite instantiations:

\begin{gather*}
    \begin{aligned}
     & \al, \bt \Vdash \al \le 1 \to \bt \Vdash \bt \le 1 \to \al\\
\rto~& \al, \bt[1], \bt[2], \{\} \le \bt \le \{\bt[1] \to \bt[2]\}
        \Vdash \al \le 1 \to \bt \Vdash \bt[1] \to \bt[2] \le 1 \to \al\\
\rto~& \cdots\\
\rto~& \al, \{1\} \le \bt[1] \le \{\}, \{\} \le \bt[2] \le \{\al\},
        \{\} \le \bt \le \{\bt[1] \to \bt[2]\}
        \Vdash \al \le 1 \to \bt\\
\rto~& \cdots\\
    \end{aligned}
\end{gather*}

% \jimmy{Not sure whether this discussion is valueable;
% in fact the example is hard to demonstrate the infinite loop
% (not straightforward to understand)}

A further analyse on the problem suggests that the \emph{loop dependencies} are not detected
and reduced smartly by the lazy algorithm:
it insist too much on not missing any possible solutions.
By loop dependency we refer to the following pattern
$\al \le A \to B \text{ where } \al \in A \to B$
In the above example, one can derive (by transitivity of subtyping)
$\al \le 1 \to \bt \le 1 \to 1 \to \al$
which forms a (indirect) loop dependency on $\al$.

By performing experimental implementations and randomly generated tests,
we observe that the algorithm accepts more valid relations when there are no loop dependencies,
but may be trapped in infinite loops otherwise.
We have also tried to formalize how a loop detection procedure might be used to help
prevent the non-termination problem,
but most of them turns out to be rather complicated and hard to machanically formalize.


