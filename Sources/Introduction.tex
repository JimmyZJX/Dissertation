%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Type Systems and Type Inference Algorithms}

Statically typed programming languages are widely used nowadays.
Programs are categorized by various types before they are compiled and executed.
Type errors caught before execution usually indicate potential bugs,
letting the programmers realize and correct such errors in advance.

In the early stages, programming languages like Java (before 1.5)
were built on a simple type system,
where only features like
primitive types, explicitly-typed functions,
and non-generic classes are supported.
People soon realized the need to generalize similar programs
that have different types when used.
For example, a simple way to define a function that swaps
the first two items in an integer array is
\begin{verbatim}
  void swap2(int[] arr) {
    int t = arr[0];
    arr[0] = arr[1];
    arr[1] = t;
  }
\end{verbatim}
Similarly, the swap function for a float array can be defined as
\begin{verbatim}
  void swap2(float[] arr) {
    float t = arr[0];
    arr[0] = arr[1];
    arr[1] = t;
  }
\end{verbatim}
which mostly shares the same body with the above definition for integer array,
except that the type of element in the array changes from \verb|int| to \verb|float|.
If such functionality is a commonly used one,
such as the sorting function,
we definitely want to define it once and use it on many types,
such as int, float, double, String, etc.
Luckily, later versions of Java (1.5 and above)
provides a way to define a \emph{generic} function that accepts input of different types:
\begin{verbatim}
  <T> void swap2_generic(T[] arr) {
    T t = arr[0];
    arr[0] = arr[1];
    arr[1] = t;
  }
\end{verbatim}
where \verb|T| denotes a generic type that programmers may arbitrarily pick.
The \verb|swap2_generic| function utilizes the feature of Java's type system, generics,
to improve modularity.
The following program invokes the generic function (suppose we defined
the \verb|swap2_generic| function in a \verb|Utils| class as a static method)
\begin{verbatim}
  Double[] arr = new Double[2];
  arr[0] = 1.0; arr[1] = 2.0;
  Utils.<Double>swap2_generic(arr);
  System.out.println(arr[0] + " " + arr[1]);
\end{verbatim}

However, the type parameter \verb|<Double>| seems a bit redundant:
given the type of \verb|arr| is \verb|Double[]|,
the generic variable \verb|T| can only be \verb|Double|.
In fact, with the help of \emph{type inference},
we can simply write
\begin{verbatim}
  Utils.swap2_generic(arr);
\end{verbatim}
to call the function.
When compiling the above code,
type inference algorithms help programmers to fill in the missing type automatically,
thus saving them from writing redundant code.

From the example above, we learn that the generics of Java
together with type inference algorithms used in the compiler
help programmers to write generic functions,
given that the functionality is generic in nature.
In other words, good features introduced to type systems
accept more meaningful programs.

On the other hand, being able to accept all syntactically correct programs,
as dynamically-typed programming languages do,
is not desirable as well.
Ill-typed programs are easy to write by mistake, like \verb|"3" / 3|;
or even well-typed programs with ambiguous/unexpected meaning like \verb|"3" + 4 + 5|
(for someone who is not familiar with the conventions, she might think this evaluates to "39").
Or even more commonly seen in practice,
an accidentally misspelled variable name might cause a runtime error until
the line of code is actually executed.
Type systems are designed to prevent such problems from happening,
therefore statically-typed languages
ensure type-safety, or ``well-typed programs cannot go wrong''.
Type inference algorithms that come along with modern type systems
help eliminate trivial or redundant parts of programs to
improve the conciseness.

\subsection{Functional Programming and System F}
Nowadays, more and more programming languages adopt the functional programming paradigm,
where functions are first-class citizens,
and programs are mostly constructed with function applications.
Functional programming originates from the
lambda calculus~\citep{alonzo1932lambda}.
The simply-typed lambda calculus~\citep{pierce2002types} extends the lambda calculus with
a simple static type checking algorithm,
preventing ill-typed programs before actual execution.
However, the system does not allow polymorphic functions and
thus is quite tedious to express higher-level logic.

In order to improve the expressiveness of functional programming languages,
System F~\citep{reynolds1983types} introduces polymorphism
via the universal quantifier $\forall$ for types
and the $\Lambda$ binder (to introduce type-level functions) for expressions.
For example, the identity function that can range over any type of the form $A \to A$
can be encoded in System F:
$$\text{id} = \Lambda a.~\lam[x:a]x : \all a \to a$$
To enjoy the polymorphism of such a function,
one needs to first supply a type argument like $\text{id}~@\text{Int}$
(we use the $@$ sign to denote a type-level application),
so that the polymorphic function is \emph{instantiated} with
a concrete type.

\paragraph{Implicit Parametric Polymorphism}

Although being much more expressive than simply-typed systems,
plain System F is still tedious to use.
It feels a bit silly to write $\text{id}~@\text{Int}~3$ compared with $\text{id}~3$,
because the missing type is quite easy to figure out,
in presence of the argument, which is of type $\text{Int}$ and
should represent $a$ for the function application at the same time.
With \emph{implicit parametric polymorphism}~\citep{reynolds1983types},
type arguments like $@\text{Int}$ are not written by the programmer explicitly,
in contrast, it is the type inference algorithm's responsibility to guess them.

Practically speaking, in Java we can define the identity function as well,
\begin{verbatim}
  <T> T id(T x) {
    return x;
  }
\end{verbatim}
And we typically use the function without specifying the type parameters,
because the type system already supports implicit parametric polymorphism:
\begin{verbatim}
  int n = id(3);
  String s = id("3");
\end{verbatim}

Theoretically speaking, it is unfortunate that
there does not exist such a perfect algorithm that
can automatically guess missing type applications for every possible
System F program~\citep{tiuryn1996subtyping}.
For example, the following expression is ambiguous
when the implicit type argument is not given:
$$\text{f} = (\text{choose} : \all a \to a \to a)~(\text{id} : \all[b] b \to b)$$
It is unclear how the type variable is instantiated during the polymorphic application:
one possibility is that $a$ is chosen to be the type of $\text{id}$, or $\all[b] b \to b$,
resulting in the type $(\all[b] b \to b) \to (\all[b] b \to b)$
(Note that we cannot express this type in programming languages like Java,
simply because the $\forall$ quantifiers do not appear at the top level,
and its type system is already a restricted version of System F).
However, another valid type is $\all[b] (b \to b) \to (b \to b)$,
which is obtained by first instantiating $\text{id}$ with a fresh type variable $b$,
and generalizing the type after calculating the type of the application.
Furthermore, between the two possible types, neither one is better:
there exist programs that type check under either one of them and
fail to type check under the other.

The fact that implicit parametric algorithm for full System F is impossible
motivates people to discover restrictions on the type system
under which type inference algorithms are capable of guessing the best types.


\subsection{Hindley-Milner Type System}

The Hindley-Milner (henceforth denoted as HM) type system~\citep{hindley1969principal,milner1978theory,damas1982principal} restricts System F types to \emph{type schemes},
or \emph{first-order polymorphism},
where polymorphic types can only have universal quantifiers in the top level.
For example, $\all[a~b] (a \to b) \to a \to b$ is allowed,
but not $(\all[b] b \to b) \to (\all[b] b \to b)$.
The type system of many programming languages like Java adopts the idea from HM,
where generics is a syntax to express polymorphic types in HM:
all the generic variables must be declared
at the top level before the function return type.
An important property of the HM type inference algorithm is \emph{principality},
where any unannotated program can be inferred to
a most general type within its type system.
This supports full type-inference without any type annotations.

For example, the following function
$$\text{g} = \lam[x]{\lam[y] x}$$
can be assigned to types $\text{Int} \to \text{Bool} \to \text{Int}$,
$\text{Int} \to \text{Int} \to \text{Int}$ or infinitely many others.
The HM inference algorithm will infer a more general type $\all[a]{\all[b] a \to b \to a}$.
In order to use the function as the types mentioned above,
a more-general-than relation $\le$ is used to describe that
the polymorphic type can be instantiated to more concrete types:
$$\begin{aligned}
  \all[a]{\all[b] a \to b \to a} &\le \text{Int} \to \text{Bool} \to \text{Int}\\
  \all[a]{\all[b] a \to b \to a} &\le \text{Int} \to \text{Int} \to \text{Int}\\
  \all[a]{\all[b] a \to b \to a} &\le \all a \to a \to a\\
\end{aligned}$$

\paragraph{Predicativity}
In the HM system, $\forall$ quantifiers can appear only on the top level,
type instantiations will always be \emph{monotypes},
i.e. types without the $\forall$ quantifier.
We refer to such a system as \emph{predicative}.
In contrast, System F does not restrict the types to instantiate,
thus being an impredicative system.
An important challenge is that full type inference for impredicative
polymorphism is known to be undecidable~\citep{wells1999typability}.
There are works that focus on practical inference of
impredicative systems~\citep{le2003ml,leijen2008hmf,
vytiniotis2008fph,Serrano2018,quicklook2020,FreezeML}.
However, throughout this work, we study predicative type systems only.

\subsection{Higher-Ranked Polymorphism}
As functional languages evolved, the need for more expressive
power has motivated language designers to look beyond HM,
where there is still one obvious weakness that prevents some useful programs to type check:
HM only have types of rank-1, since all the $\forall$'s appear on the top level.
Thus one expected feature is to allow 
\emph{higher-ranked polymorphism} where polymorphic types can
occur anywhere in a type signature.
This enables more code reuse and more expressions to type check, and has
numerous applications~\citep{jones1995functional,gill1993short,launchbury1995state,lammel2003scrap}.

One of the interesting examples is the ST monad~\citep{94lazyST} of Haskell,
where the \verb|runST| function is only possible to express in a rank-2 type system:
$$\text{runST} :: \all[a]{(\all[s]{\text{ST}~s~a}) \to a}$$
The type is rank-2 because of the inner $\forall$ quantifier
in the argument position of the type.
Such a type encapsulates the state and makes sure
that program states from different computation threads do not escape their scopes,
otherwise the type checker should reject in advance.

In order to support higher-ranked types, we need to extend the type system of HM,
but not taking a too big step since type inference for full System F would be impossible.
A simple polymorphic subtyping relation
proposed by \citet{odersky1996putting}
extends the HM system by allowing higher-ranked types,
but instantiations are still limited to monotypes,
thus the system remains predicative.

\subsection{Bidirectional Typing}

In order to improve the expressiveness for higher-ranked systems,
some type annotations are necessary to guide type inference.
In response to this challenge, several decidable type systems requiring some annotations 
have been proposed~\citep{dunfield2013complete,
jones2007practical,Serrano2018,le2003ml,leijen2008hmf,vytiniotis2008fph}.

As an example,
$$\text{hpoly} = \lam[(f:\all a \to a)] (f~1, f~'c')$$
the type of $\text{hpoly}$ is $(\all a \to a) \to (\text{Int}, \text{Char})$,
which is a rank-2 type and is not typeable in HM.
Notably (and unlike Hindley-Milner) the lambda argument $f$ requires a
\emph{polymorphic} type annotation.
This annotation is needed because the single universal quantifier
does not appear at the top-level. Instead, it is used to quantify a
type variable $a$ used in the first argument of the function.

One of the key features that improve type inference algorithms with type annotations
is bidirectional typing~\citep{pierce:local}, a technique that combines two modes of typing:
type checking, which checks an expression against a given type,
and type synthesis (inference), which infers a type from an expression.
Bidirectional typing is quite useful when the language supports type annotations,
because those ``hints'' are handled with the checking mode.
With bidirectional type-checking the typing judgment has two modes.
In the checking mode $\Gm \vdash e \Lto A$ both $e$ and $A$ are inputs:
i.e. we check whether expression $e$ has some type $A$.
In the synthesis mode $\Gm \vdash e \To A$ only $e$ is an input:
we need to calculate the output type $A$ from the input expression $e$.
% For the typing judgment $\Gm \vdash e : A$,
% the input of checking mode ($e \Lto A$) both $e$ and $A$,
% while synthesis mode ($e \To A$)
% needs to calculate the output $A$ from the input expression $e$.
It is clear that, with less information,
the synthesis mode is more difficult to implement than the checking mode.
Therefore, bidirectional type checking with type annotations
provides a way for the programmer to guide
the type inference algorithm when the expression is tricky to analyse,
especially in the case where higher-ranked types are involved.
In addition, bidirectional typing algorithms
improve the quality of error messages in practice,
due to the fact that they report errors in a relatively local range,
compared with global unification algorithms.

Two closely related type systems that 
support \emph{predicative} higher-ranked type inference were proposed 
by Peyton Jones et al.~\citep{jones2007practical} and Dunfield and
Krishnaswami~\citep{dunfield2013complete} (henceforth denoted as DK). 
These type systems are
popular among language designers and their ideas have been adopted by
several modern functional languages, including Haskell, PureScript~\citep{PureScript} and
Unison~\citep{Unison} among others.

DK developed a higher-ranked global bidirectional type system
based on the declarative system by Odersky and L\"aufer~\citep{odersky1996putting}.
Beyond the existing works, they introduce a third form of judgment,
the application inference $\appInf{A}{e}{C}$,
where the function type $A$ and argument expression $e$ are input,
and type $C$ is the output representing the result type
of the function application $(f :: A)~e$.
Note that $f$ does not appear in this relation,
and one can tell the procedure for type checking application expressions
from the shape of the judgment ---
firstly, the type of the function is inferred to $A$;
then the application inference judgment is reduced to a checking judgment
to verify if the argument $e$ checks against the argument part of $A$;
finally, output the return part of $A$.
Formally, the rules implement the procedure we described above:
$$
\inferrule*[right=$\mathtt{Decl{\to} E}$]
    {\Psi\vdash e_1\To A \quad \Psi\vdash \appInf{A}{e_2}{C}}
    {\Psi\vdash e_1~e_2 \To C}
\qquad
\inferrule*[right=$\mathtt{Decl{\to}App}$]
    {\Psi\vdash e \Lto A}
    {\Psi\vdash \appInf{A \to C}{e}{C}}
$$
The use of application inference judgment avoids implicit instantiations
of types like HM, instead,
when the function type $A$ is a polymorphic type,
it is explicitly instantiated by the application inference until it becomes a function type:
$$\inferrule*[right=$\mathtt{Decl\forall App}$]
    {\Psi \vdash \tau \quad \Psi\vdash \appInf{[\tau/a]A}{e}{C} }
    {\Psi\vdash \appInf{\all A}{e}{C}}$$
As a result, DK is in a more syntax-directed system compared with HM-like systems.

DK also provided an elegant formalization of their sound and complete algorithm,
which has also inspired implementations of type inference in some polymorphic 
programming languages (such as PureScript~\citep{PureScript} or DDC~\citep{Disciple}).


The focus of this thesis is also
on predicative implicit higher-ranked bidirectional type inference algorithms.


\subsection{Subtyping}

 The term ``subtyping'' is used as two slightly different concepts in this thesis.
 One of them refers to the polymorphic subtyping relation used in polymorphic type systems,
 which compares the degree of polymorphism between types, i.e. the more-general-than relation.
 Chapters \ref{chap:ITP} and \ref{chap:ICFP} only focus on this type of subtyping relation.
 The other one is the subtyping usually seen in object-oriented programming,
 where there are some built-in or user-defined type conversion rules.

Type system in presence of object-oriented-style subtyping allows
programmers to abstract functionalities inside a class
and thus benefit from another form of polymorphism.
Introduced by~\citet{Mitchell1984,Reynolds1985,Cardelli1988},
subtyping is studied for explicitly typed programs.
Compared with functional programming languages,
mainstream object-oriented languages lacks competitive type inference algorithms.
We will further introduce object-oriented subtyping as a feature in Chapter~\ref{chap:Top},
where we also introduce the top and bottom types as the minimal support for subtyping.

\section{Mechanical Formalizations and Theorem Provers}

Although type inference is important in practice and
receives a lot of attention in
academic research, there is little work on mechanically formalizing
such advanced forms of type inference in theorem provers.
The remarkable exception is work done on the formalization of 
certain parts of Hindley-Milner type inference~\citep{naraschewski1999type,
dubois2000proving,dubois1999certification,urban2008nominal,
garrigue2015certified}. However,
there is still no formalization of the higher-ranked type systems
that are employed by modern languages like Haskell.
This is at
odds with the current trend of mechanical formalizations in
programming language research. In particular, both the POPLMark
challenge~\citep{aydemir2005mechanized} and
CompCert ~\citep{leroy2012compcert} have significantly promoted
the use of theorem provers to model various aspects of programming
languages. Today, papers in various programming language venues routinely
use theorem provers to mechanically formalize: \emph{dynamic and
  static semantics} and their correctness properties~\citep{aydemir2008engineering},
\emph{compiler correctness}~\citep{leroy2012compcert}, \emph{correctness of
  optimizations}~\citep{Bertot04}, \emph{program analysis}~\citep{Chang2006}
or proofs involving \emph{logical relations}~\citep{abel2018}.

\paragraph{Motivations for Mechanical Formalizations.}
The main argument for mechanical formalizations is a simple one.
Proofs for programming languages tend to be \emph{long}, \emph{tedious},
and \emph{error-prone}. In such proofs, it is very easy to make mistakes
that may invalidate the whole development.
Furthermore, readers and reviewers often do not have time to
look at the proofs carefully to check their correctness.
Therefore errors can go unnoticed for a long time.
In fact, manual proofs are commonly observed to have flaws
that invalidate the claimed properties.
For instance, \citet{KleinRunYourResearch} reproduced the proofs of
nine ICFP 2009 papers in Redex, and found problems in each one of them.
We also found false lemmas and incorrect proofs
in DK's manual proof~\citep{dunfield2013complete}.
Mechanical formalizations provide, in principle, a natural
solution for these problems. Theorem provers can automatically check and
validate the proofs, which removes the burden of checking from both
the person doing the proofs as well as readers or reviewers.

Moreover, extending type-inference algorithms with
new programming language features is often quite
delicate. Studying the meta-theory for such extensions would be
greatly aided by the existence of a mechanical
formalization of the base language,
which could then be reused and extended by the language designer.
Compared with manual proofs which may take a long time before
one can fully understand every detail,
theorem provers can quickly point out
proofs that are invalidated after extensions.


\paragraph{Challenges in Variable Binding and Abella.}
Handling variable binding is particularly challenging in type inference,
because the algorithms typically do not rely simply on local environments,
but instead propagate information across judgments.
Yet, there is little work on
how to deal with these complex forms of binding in theorem provers.
We believe that this is the primary reason
why theorem provers have still not been widely
adopted for formalizing type-inference algorithms.

The Abella theorem prover~\citep{AbellaDesc}
is one that specifically eases formalization onbinders.
Two common treatments of binding
are to use the De Bruijn indices~\citep{DEBRUIJN1972381}
and the nominal logic framework of Pitts~\citep{PITTS2003165}.
In contrast, Abella uses the abstraction operator
in a typed $\lambda$-calculus to encode binding.
Its $\lambda$-tree syntax, or HOAS, and features including the $\nabla$ quantifier
and higher-order unification, allow for better experiences than using Coq libraries
utilizing other approaches.
In practice, Abella uses the $\nabla$ (nabla) quantifier and nominal constants to help
quantify a ``fresh'' variable during formalization.
For example, the common type checking rule
$$\inferrule*{e \Lto A \\ a \text{ fresh}}{e \Lto \all A}$$
is encoded as
\begin{abella}
  check E (all A) := nabla a, check E (A a)
\end{abella}
in Abella, where the $\nabla$ quantifier introduces a fresh type variable $a$
and later use it to ``open'' the body of $\all A$.

Throughout the thesis, all the type systems and declared properties are
mechanically formalized in Abella.


% \section{Our Proposals}
% remove?

\section{Contributions and Outline}

\paragraph{Contributions}

In this thesis, we propose variants of type inference algorithms
for higher-ranked polymorphic type systems
and formalize each of them in the Abella theorem prover.
It is the first work on higher-ranked type inference
that comes with mechanical formalizations.
In summary, the main contributions of this thesis are:


\begin{itemize}
  \item {\bf Chapter~\ref{chap:ITP}} presents a predicative polymorphic subtyping algorithm.
  \begin{itemize}
    \item We proved that our algorithm is \emph{sound},
      \emph{complete}, and \emph{decidable}
      with respect to OL's higher-ranked subtyping specification
      in the Abella theorem prover.
      And we are the first to formalize the meta-theoretical results of
      a polymorphic higher-ranked subtyping algorithm.
    \item
      Similar to DK's algorithm, we employ an ordered context to collect
      type variables and existential variables (used as placeholders for guessing monotypes).
      However, our unification process is novel.
      DK's algorithm solves variables on-the-fly and communicates
      the partial solutions through an output context.
      In contrast, our algorithm collects a list of judgments and propagate partial solutions
      across them via eager substitutions.
      Such technique eliminates the use of output contexts,
      and thus simplifies the metatheory and makes mechanical formalizations easier.
      Besides, using only a single context keeps the definition of well-formedness simple,
      resulting in an easy and elegant algorithm.
  \end{itemize}

  \item Chapter~\ref{chap:ICFP} presents a new
    bidirectional higher-ranked typing inference algorithm based on DK's declarative system.
    \begin{itemize}
      \item We are the first to present a full mechanical formalization for
        a type inference algorithm of higher-ranked type system.
        The \emph{soundness}, \emph{completeness}, and \emph{decidability} are shown
        in the Abella theorem prover,
        with respect to DK's declarative system.
      \item We propose \emph{worklist judgments},
        a new technique that unifies ordered contexts and judgments.
        This enables precise scope tracking of variables in judgments
        and avoids the duplication of context information across judgments in worklists.
        Similar to the previous algorithm, partial solutions are propagated across
        judgments in a single list consist of both variable bindings and judgments.
        Nevertheless, the unification of worklists and contexts exploits the fact
        that judgments are usually sharing a large part of common information.
        And one can easily tell when a variable is no longer referred to.
      \item Furthermore, we support inference judgments so that bidirectional typing can be
        encoded as worklist judgments.
        The idea is to use a continuation passing style to
        enable the transfer of inferred information across judgments.
    \end{itemize}

  \item Chapter~\ref{chap:Top}
    further extends the higher-ranked system with object-oriented subtyping.
    \begin{itemize}
      \item We propose a bidirectional declarative system extended with
        the top and bottom types and relevant subtyping and typing rules.
        Several desirable properties are satisfied and mechanically proven.
      \item A new backtracking-based worklist algorithm is presented and
        proven to be \emph{sound} with respect to our declarative specification
        in the Abella theorem prover.
        Extended with subtyping relations of the top and bottom types,
        simple solutions such as $\top$ or $\bot$ satisfies subtyping constraints
        in parallel with other solutions witch does not involve object-oriented subtyping.
        Our backtracking technique is specifically well-suited for the
        non-deterministic trial without missing any of them.
      \item We also formalize the rank-1 restriction of subtyping relation,
        and proved that our algorithmic subtyping is \emph{complete} under such restriction.
    \end{itemize}
\end{itemize}



\paragraph{Prior Publications.}
This thesis is partially based on the publications by the author~\citep{itp2018,icfp2019},
as indicated below.
\begin{description}
\item Chapter~\ref{chap:ITP}: Jinxu Zhao, Bruno C. d. S. Oliveira, and Tom Schrijvers. 2018.
  ``Formalization of a Polymorphic Subtyping Algorithm''.
  In \emph{International Conference on Interactive Theorem Proving (ITP)}.
\item Chapter~\ref{chap:ICFP}: Jinxu Zhao, Bruno C. d. S. Oliveira, and Tom Schrijvers. 2019.
  ``A Mechanical Formalization of Higher-Ranked Polymorphic Type Inference''.
  In \emph{International Conference on Functional Programming (ICFP)}.
\end{description}

\paragraph{Mechanized Proofs.}
All proofs in this thesis is mechanically formalized in the Abella theorem prover
and are available online:
\begin{description}
  \item Chapter~\ref{chap:ITP}: \url{https://github.com/JimmyZJX/Abella-subtyping-algorithm}
  \item Chapter~\ref{chap:ICFP}: \url{https://github.com/JimmyZJX/TypingFormalization}
  \item Chapter~\ref{chap:Top}: \url{https://github.com/JimmyZJX/TODO}
  \jimmy{TODO URL}
\end{description}

\noindent\makebox[\linewidth]{\rule{0.7\textwidth}{0.4pt}}

\vspace{1.5\baselineskip}



