%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
\label{chap:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Throughout the thesis, we have already discussed much of the closest related work.
In this section we summarize the key differences and novelties,
and discuss some other related work.

\section{Higher-Ranked Polymorphic Type Inference Algorithms}

\subsection{Predicative Algorithms}

Higher-ranked polymorphism is a convenient and practical feature of
programming languages.  Since full type-inference for System F is
undecidable~\citep{wells1999typability}, various decidable partial
type-inference algorithms were developed.
% No need to cite here: the following text is describing the point.
The declarative formulation of subtyping in Chapters~\ref{chap:ITP} and \ref{chap:ICFP}
(and later extended in Chapter~\ref{chap:Top}),
originally proposed by \citet{odersky1996putting}, is \emph{predicative}:
$\forall$'s only instantiate to monotypes.  The monotype restriction
on instantiation is considered reasonable and practical for most
programs, except for those that require sophisticated forms of
higher-order polymorphism.
In those cases, type annotations may guide the type system to
accept lambda functions of higher-ranked types.

In addition to OL's type system,
the bidirectional system proposed by DK~\citep{dunfield2013complete}
accepts even better type annotations, which also allow polymorphic types.
Such annotations also improve readability of the program,
and are not much of a burden in practice.
DK's algorithm is shown to be sound, complete and decidable in 70 pages of manual proofs.
Though carefully written, some of the proofs are incorrect
(see discussion in Section~\ref{ssec:DK_Algorithm} and
\ref{sec:metatheory:non-overlapping}),
which creates difficulties when formalizing them in a proof assistant.
In their follow-up work \citet{DunfieldIndexed} enrich the bidirectional higher-ranked system with
existentials and indexed types.
With a more complex declarative system, they developed a proof of over 150 pages.
It is even more difficult to argue its correctness for every single detail
within such a big development.
Unfortunately, we find that their Lemma 26 (Parallel Admissibility) appears to have the same issue 
as lemma 29 in \citep{dunfield2013complete}: the conclusion is false. We also discuss
the issue in more detail in Section~\ref{ssec:DK_Algorithm}.

\citet{jones2007practical} developed another higher-ranked predicative bidirectional type system.
Their subtyping relation is enriched with \emph{deep skolemisation},
which is more general than ours and allows more valid relations.
For example,
$$\all{\all[b] a \to b \to b} \le \all a \to (\all[b] b \to b)$$
this subtyping relation does not hold in OL's subtyping relation,
because the instantiation of $b$ in the left-hand-side type
happens strictly before the introduction of
the variable $b$ in the right-hand-side type.
Deep skolemisation can be achieved through
a pre-processing that extracts all the $\forall$'s
in the return position of a function type to the top level.
After such pre-processing, the right-hand-side type becomes
$\all {\all[b] a \to b \to b}$ and the subtyping relation holds.

In terms of handling applications where type parameters are implicit,
they do not use the application inference judgment as DK's type system.
Instead, they employ a complicated mechanism for implicit instantiation
taken care by the unification process for the algorithm.
A manual proof is given, showing that the algorithm is sound and
complete with respect to their declarative specification.

In a more recent work, \citet{xie2018letarguments} proposed a variant of a
bidirectional type inference system for a predicative system with higher-ranked types.
Type information flows from arguments to
functions with an additional \emph{application} mode. This variant 
allows more higher-order typed programs to be inferred without additional annotations.
Following the new mode, the let-generalization of the Hindley-Milner system
is well supported as syntactic sugar. The formalization includes some
mechanized proofs for the declarative type system, but all proofs regarding
the algorithmic type system are manual.

\subsection{Impredicative Algorithms}

Impredicative System F allows instantiation with polymorphic types,
but unfortunately its subtyping system is already undecidable~\citep{tiuryn1996subtyping}.
Works on partial impredicative type-inference algorithms
navigate a variety of design tradeoffs for a decidable algorithm.
Generally speaking, such algorithms tend to be more complicated,
and thus less adopted in real-world programming languages.

$\text{ML}^\text{F}$~\citep{le2003ml,remy2008from,Botlan2009recasting}
extends types of System F with a form of bounded quantification
and proposes an impredicative system.
The type inference algorithm always infers principle types given proper type annotations.
Through the technique called ``monomorphic abstraction of polymorphic types'',
polymorphic instantiations are expressed by constraints
on type variables of type schemes.
An annotation is only needed when an argument of a lambda function
is used polymorphically.
Moreover, their type system is robust against a wide range of program transformations,
including let-expansion, let-reduction and $\eta$-expansion.
However, the extended type structure of $\text{ML}^\text{F}$
introduces non-compatible types with System F,
and it complicates the metatheory and implementation of the type system.

HMF~\citep{leijen2008hmf} takes a slightly different approach
by not extending types with bounds,
therefore programmers still work with plain System F types.
The algorithm works similarly compared to $\text{ML}^\text{F}$,
except that it does not output richer types.
Instead, when there are ambiguity and no principal type can be inferred,
being a conservative algorithm,
HMF prefers predicative instantiations
to maintain backward the compatibility of HM.
Soon after HMF, HML~\citep{leijen2009flexible} is proposed as
an extension of HMF with flexible types.
The system eliminates rigid quantifications ($\alpha = \sigma$)
of $\text{ML}^\text{F}$ and only keeps the flexible quantifications ($\alpha \ge \sigma$).
Furthermore, a variant ``Rigid HML'' is presented
by restricting let-bindings to System F types only,
so that type annotations can still stay inside System F.
Similar to HMF, they both require type annotations at higher-ranked
types or ambiguous implicit instantiations.

FPH~\citep{vytiniotis2008fph} employ a box notion \framebox{$\sigma$} to
encapsulate polymorphic instantiations internally in the algorithm.
A type inside a box indicates that the instantiation is impredicative.
Therefore an inferred type with a box type in it means that
incomparable System F types may be produced,
and that is rejected before entering the environment.
Although more annotations might be required compared with other approaches,
the algorithm of FPH is much simpler,
thanks to the simple syntax and subtyping relation of box types.


Guarded Impredicative Polymorphism~\citep{Serrano2018} was recently proposed
as an improvement on GHC's type inference algorithm with impredicative instantiation.
They make use of local information in $n$-ary applications to
infer polymorphic instantiations with a relatively simple specification and unification algorithm.
Although not all impredicative instantiations can be handled well,
their algorithm is already quite useful in practice.

A recent follow-up work, the Quick Look~\citep{quicklook2020},
takes a more conservative approach.
The algorithm will try its best to infer impredicative instantiations,
and only use the instantiation when it is the best one.
In order to do so,
the algorithm also needs to analyse all the argument types during a function call,
and make use of the \emph{guardedness} property to decide the principality of
the inferred instantiation.
When Quick Look cannot give the best instantiation,
it will instead try predicative inference as HM.
This conservative approach makes sure that the algorithm does not infer bad types
and leads the subsequent type checking in wrong directions.
In the meantime, they treat the function arrow ($\to$)
as invariant like normal type constructors.
This change significantly restricts the subtyping relation and thus
simplifies guesses for implicit parameters.
Manual $\eta$-expansions are required to regain
the co- and contravariance of function types.

FreezeML~\citep{FreezeML} is another recent work that extends the ML type system
with impredicative instantiations.
A special syntax of expression, the explicit freezing $\lceil x \rceil$,
is added to guide the type system.
Freezed variables are prevented to be instantiated by the type system,
therefore variables of polymorphic types may force the type inference algorithm
to instantiate impredicatively.
In combination with the let-generalization rule, programmers may also encode
explicit generalization and explicit instantiation.
This work provides a nice means for programmers to control how type inference algorithm
deals with impredicative instantiations.

\section{Type Inference Algorithms with Subtyping}
Type systems in presence of subtyping usually encounter constraints that
are not simply equalities as in HM.
Therefore constraint solvers used in HM, where unifications are based on equality,
cannot be easily extended to support subtyping.
Instead, constraints are usually collected as subtyping relations
and may delay resolving as the constraints accumulate.
\citet{EIFRIG1995,Eifrig1995sound} proposed systems that are based on
\emph{constraint types}, i.e. types expressed together with a set of constraints
$\tau \mid \{\overline{\tau_1 \le \tau_2}\}$.
Their type checking algorithm checks at each step whether
each constraint in the closure of constraint set is \emph{consistent},
which is a set of rules that prevent obvious contradictions appear,
such as $\text{Int} \le \text{Bool}$.
Our attempt in Section~\ref{subsec:lazy_subst} is similar to this idea,
where we exhaustively check if every subtyping relation derived from the bounds is valid.
However, our algorithm tries to solve the bounds into concrete types,
and it turns out that it never terminates in some complex cases.

Constraint types improve the expressiveness of the type system,
yet the type inference algorithm can be quite slow.
The size of the constraint is linear in the program size,
and the closure can grow to cubic size.
\citet{pottier1998phd} proposed three methods to simplify constraints in his Ph.D. thesis,
aiming at improving the efficiency of type inference algorithms and
improving the readability of the resulting types.
By \emph{canonization}, constraints are converted to canonical forms
with the introduction of new meta-variables;
\emph{garbage collection} eliminates constraints that do not affect the type;
finally, \emph{minimization} shares nodes within the constraint graph.

Inspired by the simplification strategies of Pottier's,
MLsub~\citep{mlsub} suggest that the data flow on the constraint graph
can reflect directly on types extended by a richer type system,
where lattices operations are used to represent constraints imposed on type variables.
Polar types distinguish between input types and output types,
and pose different restrictions on them.
As a result, constraints can easily be transformed into canonical forms,
and the bi-unification algorithm can solve them by simple substitutions.
One can also view the MLsub system as a different way to encode the constraint types:
instead of a set of constraints that is stated along with the type,
types themselves now contain subtyping constraints with the help of lattice operations.
Similarly, the size of constraints may grow with the size of program
and affect the readability.
Therefore, various simplification algorithms should be used in real applications.
A more recent work, the Simple-sub~\citep{Parreaux2020simple}, further simplifies
the algorithm of MLsub and is implemented in 500 lines of code.
While being equivalent to MLsub, it is a more efficient variant.

\section{Techniques Used in Type Inference Algorithms}

\subsection{Ordered Contexts in Type Inference}
\citet{gundry2010type} revisit algorithm $\mathcal{W}$ and
propose a new unification algorithm with the help of ordered contexts.
Similar to DK's algorithm, information of meta-variables flows from input contexts to output contexts.
Not surprisingly, its information increase relation has a similar role to DK's context extension.
Our algorithm, in contrast,
eliminates output contexts and solution records ($\al = \tau$),
simplifying the information propagation process through immediate substitution
by collecting all the judgments in a single worklist.

\subsection{The Essence of ML Type Inference}
Constraint-based type inference is adopted by \citet{remy-attapl} for
ML type systems, which do not employ higher-ranked polymorphism. An
interesting feature of their algorithm is that it keeps precise
scoping of variables, similarly to our approach.  Their algorithm is
divided into constraint generation and solving phases (which are
typical of constraint-based algorithms). Furthermore an intermediate
language is used to describe constraints and their constraint solver
utilizes a stack to track the state of the solving process.  In
contrast, our algorithm has a single phase, where the judgment chains
themselves act as constraints, thus no separate constraint language is
needed.

%A possible advantage of a separate constraint language is to provide
%a general and reuseable interface
%that interprets the desired properties for type inference.

\subsection{Lists of Judgments in Unification}
Some work~\citep{Reed2009,Abel2011higher} adopts a similar idea to this paper
in work on unification for dependently typed languages. Similarly to our work
the algorithms need to be very careful about scoping, since the order of variable
declarations is fundamental in a dependently typed setting. 
Their algorithms simplify a collection of unification constraints progressively in a single-step style.
In comparison, our algorithm mixes variable declarations with judgments,
resulting in a simpler judgment form,
while processing them in a similar way.
One important difference is that contexts are
duplicated in their unification judgments, which complicates the unification process,
since the information of each local context needs to be synchronized.
Instead we make use of the nature of ordered context to control the scope of unification variables.
While their algorithms focus only on unification,
our algorithm also deals with other types of judgments like synthesis.
A detailed discussion is given in Section~\ref{sec:overview:list}.





\section{Mechanical Formalization of Polymorphic Type Systems}

% \paragraph{Mechanical Formalization of Polymorphic Subtyping}
% In all previous work on type inference for higher-ranked polymorphism
% (predicative and impredicative) discussed above, proofs and
% metatheory for the algorithmic aspects are manual. The only partial effort on mechanizing algorithmic
% aspects of type inference
% for higher-ranked types is
% the Abella formalization of \emph{polymorphic subtyping} by \citet{itp2018}.
% The judgment form of worklist $\Gm \vdash \Omega$ used in the formalization simplifies
% the propagation of existential variable instantiations.
% However, the approach has two main drawbacks:
% it does not collect unused variable declarations effectively;
% and the simple form of judgment cannot handle inference modes, which output types.
% The new worklist introduced in this paper inherits the simplicity of propagating instantiations,
% but overcomes both of the issues by mixing judgments with declarations
% and using the continuation-passing-style judgment chains. Furthermore,
% we formalize the complete bidirectional type system by
% \citet{dunfield2013complete}, whereas Zhao et al. only formalize
% the subtyping relation. 

% \paragraph{Mechanical Formalizations of Other Type-Inference Algorithms}
Since the publication of the \textsc{POPLMark} challenge~\citep{aydemir2005mechanized},
many theorem provers and packages provide new methods for dealing
with variable binding~\citep{aydemir2008engineering,urban2008nominalTech,chlipala2008parametric}.
More and more type systems are formalized with these tools.
However, mechanizing certain algorithmic aspects, like unification and
constraint solving, has received very little attention and is still challenging.
Moreover, while most tools support local (input) contexts in a neat way,
many practical type-inference algorithms require
more complex binding structures with output contexts or various forms of constraint solving procedures.

Algorithm $\mathcal{W}$,
as one of the classic type inference algorithms for polymorphic type systems,
has been manually proven to be sound and complete
with respect to the Hindley-Milner type system~\citep{hindley1969principal,milner1978theory,damas1982principal}.
After around 15 years, the algorithm was formally verified by
\citet{naraschewski1999type} in Isabelle/HOL~\citep{nipkow2002isabelle}.
The treatment of new variables was tricky at that time, while the overall structure follows the
structure of Damas's manual proof closely.
%Another complication is the encoding of substitutions,
%which they chose to formalize as a function instead of an association list.
Later on, other researchers~\citep{dubois2000proving,dubois1999certification}
formalized algorithm $\mathcal{W}$ in Coq~\citep{Coq}.
Nominal techniques~\citep{urban2008nominalTech} in Isabelle/HOL have been
developed to help programming language formalizations, and are used for a similar
verification~\citep{urban2008nominal}. Moreover, Garrigue~\citep{garrigue2015certified}
mechanized a type inference algorithm,
with the help of locally nameless~\citep{LocallyNameless},
for Core ML extended with structural polymorphism and recursion.

